# ğŸ“§ Email Spam Classifier using Naive Bayes

A simple and effective **Machine Learning project** that detects whether an email is **Spam or Not Spam** using:

* ğŸ§  Naive Bayes Algorithm
* ğŸ”¢ Count Vectorization (Bag of Words)
* ğŸ Python + Scikit-learn

Perfect for beginners in **NLP and ML projects**.

---

## ğŸš€ Project Overview

Email spam filtering is a classic NLP problem where the goal is to classify emails into:

* âŒ Spam (Promotional, malicious)
* âœ… Ham (Normal email)

This project builds a lightweight and fast spam classifier using **probabilistic learning**.

---

## ğŸ§  Technologies Used

* Python
* Scikit-learn
* Pandas
* NumPy
* CountVectorizer (BoW)
* Multinomial Naive Bayes

---

## ğŸ“Œ Machine Learning Pipeline

```
Raw Emails
   â†“
Text Cleaning
   â†“
Count Vectorization
   â†“
Naive Bayes Training
   â†“
Spam Prediction
```

---

## ğŸ” Key Concepts

### ğŸ”¢ Count Vectorization

Converts text into numerical vectors by counting word occurrences.

Example:

| Email          | free | win | hello |
| -------------- | ---- | --- | ----- |
| free win money | 1    | 1   | 0     |

Creates a **Document-Term Matrix**.

---

### ğŸ§  Naive Bayes

A probabilistic classifier based on **Bayes Theorem** with an independence assumption.

Formula:

```
P(Class | Text) âˆ P(Class) Ã— Î  P(word | Class)
```

Fast and works great for text classification.

---

## ğŸ“Š Dataset

You can use:

* SMS Spam Collection Dataset
* Enron Email Dataset
* Custom CSV dataset

Example format:

| label | message          |
| ----- | ---------------- |
| spam  | Win money now!!! |
| ham   | Meeting at 5 PM  |

---


## ğŸ§ª Model Training Steps

1. Load dataset
2. Clean text (lowercase, remove symbols)
3. Convert text â†’ vectors using CountVectorizer
4. Train Multinomial Naive Bayes
5. Evaluate accuracy

---

## ğŸ“ˆ Model Evaluation

Common metrics used:

* Accuracy
* Precision
* Recall
* F1 Score
* Confusion Matrix

---

## ğŸ“Š Example Output

```
Input: "Congratulations! You won a free iPhone"
Prediction: SPAM ğŸš¨

Input: "Let's meet tomorrow at office"
Prediction: HAM âœ…
```

---

## ğŸ“‚ Project Structure

```
email-spam-classifier/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ spam.csv
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ spam_classifier.ipynb
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ preprocess.py
â”‚   â”œâ”€â”€ train.py
â”‚   â””â”€â”€ predict.py
â”œâ”€â”€ model/
â”‚   â””â”€â”€ spam_model.pkl
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ§  Why Naive Bayes?

* Extremely fast âš¡
* Works well with small datasets
* Great baseline for NLP tasks
* Low computational cost

---

## ğŸ‘ Advantages

* Simple implementation
* High performance on text data
* Lightweight model
* Easy deployment

---

## âŒ Limitations

* Assumes feature independence
* Cannot understand context
* Performance drops with complex language

---

## ğŸ”® Future Improvements

* Use TF-IDF instead of CountVectorizer
* Add deep learning (LSTM/BERT)
* Build web app using Flask/FastAPI
* Deploy with Streamlit
* Add email API integration

---

## ğŸŒ Real-World Applications

* Gmail spam filters
* SMS spam detection
* Phishing detection
* Customer support filtering

---

## ğŸ¤ Contributing

Contributions are welcome!

1. Fork the repo
2. Create a new branch
3. Commit changes
4. Submit a Pull Request

---

## â­ If You Like This Project

* Star â­ the repo
* Fork ğŸ´ and build your own ML version
* Share with friends ğŸš€

---
